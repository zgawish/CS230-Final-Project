{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "QXGsTV4n9dXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JACNpCD2tkB4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import factorial\n",
        "import h5py\n",
        "import os\n",
        "import scipy.io as sio\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect dataset from Google Drive**"
      ],
      "metadata": {
        "id": "4mf6gCVL9i3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "path = \"drive/MyDrive/CS230_Project/Jenkins_Rstruct_Data\"\n",
        "!ls $path"
      ],
      "metadata": {
        "id": "RQtYv9EotvKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3712bb7e-8758-43f2-97e8-ef771fff7153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "JR_2015-12-04_truncated2.mat  plan_test_data.mat  plan_training_data.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Dataset information"
      ],
      "metadata": {
        "id": "G8CmWhPZ-r94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = sio.loadmat(path+\"/JR_2015-12-04_truncated2.mat\")[\"R\"][0]\n",
        "ntrials = len(R)\n",
        "shape = R[0]['spikeRaster'].todense().shape\n",
        "fields = R[0].dtype.names\n",
        "\n",
        "print(\"There are %d trials in the R-struct\" % ntrials)\n",
        "print(\"There are %d electrodes with %d millseconds of data \\n\" % (shape[0], shape[1]))\n",
        "print(\"Fields in this dataset include: \")\n",
        "for field in fields:\n",
        "    print(\"-\", field)"
      ],
      "metadata": {
        "id": "o3kdea-8t0xB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6d937a-9f2f-44b1-8e10-56628103ef44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 506 trials in the R-struct\n",
            "There are 96 electrodes with 901 millseconds of data \n",
            "\n",
            "Fields in this dataset include: \n",
            "- startDateNum\n",
            "- startDateStr\n",
            "- timeTargetOn\n",
            "- timeTargetAcquire\n",
            "- timeTargetHeld\n",
            "- timeTrialEnd\n",
            "- subject\n",
            "- counter\n",
            "- state\n",
            "- cursorPos\n",
            "- spikeRaster\n",
            "- spikeRaster2\n",
            "- isSuccessful\n",
            "- trialNum\n",
            "- timeFirstTargetAcquire\n",
            "- timeLastTargetAcquire\n",
            "- trialLength\n",
            "- target\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fields we care about the most are `target` and `spikeRaster`.\n",
        "\n",
        "The `target` field holds the coordinates of the reach direction of a given trial.\n",
        "```python\n",
        "# returns a tuple indicating the x, y, and z coordinates\n",
        "# of the target reach direction for trial 0\n",
        "R[0]['target']\n",
        "```\n",
        "The `spikeRaster` field holds a sparse matrix where each row corresponds to an electrode, and each column corresponds to a spike time. We will use the `.todense()` function to convert the row to an array of 1s and 0s where each index is a millisecond indicating whether a neuron fired (1) or not (0).\n",
        "```python\n",
        "# returns an array of spikes for the first trial and\n",
        "# and the first electrode\n",
        "R[0]['spikeRaster'].todense()[0, :]\n",
        "# returns whether there was a spike for the first trial and\n",
        "# and the first electrode during the 10th millisecond\n",
        "R[0]['spikeRaster'].todense()[0,9]\n",
        "```"
      ],
      "metadata": {
        "id": "bmUFt5hF-z-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 9 possible reach directions.\n",
        "```python\n",
        "# Targets sorted in CCW\n",
        "xy_sorted = np.array([\n",
        "             [0.0, 0.0],          # 0\n",
        "             [120.0, 0.0],          # 1\n",
        "             [84.85, 84.85],        # 2\n",
        "             [0.0,   120.0],        # 3\n",
        "             [-84.85,84.85],        # 4\n",
        "             [-120.0, 0],           # 5\n",
        "             [-84.85, -84.85],      # 6\n",
        "             [0.0, -120],           # 7\n",
        "             [84.85,-84.85]])       # 8\n",
        "```"
      ],
      "metadata": {
        "id": "wE6wrHE4pwvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restructure the data"
      ],
      "metadata": {
        "id": "QvICL4f1a5pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "JUaTbpCBqSh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = R[1:]['target']\n",
        "len(targets)"
      ],
      "metadata": {
        "id": "Lz4UPUc90MIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251cf596-a0dd-42ae-8d89-a7cc5d0e2110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "505"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_electrodes = 96\n",
        "ms = 400\n",
        "ms_start = 200\n",
        "START_SIZE = num_electrodes * ms\n",
        "# xy_sorted = np.array([\n",
        "#              [0.0, 0.0],\n",
        "#              [120.0, 0.0],\n",
        "#              [84.85, 84.85],\n",
        "#              [0.0, 120.0],\n",
        "#              [-84.85,84.85],\n",
        "#              [-120.0, 0],\n",
        "#              [-84.85, -84.85],\n",
        "#              [0.0, -120],\n",
        "#              [84.85,-84.85]])\n",
        "xy_sorted = np.array([\n",
        "             [120.0, 0.0],\n",
        "             [84.85, 84.85],\n",
        "             [0.0, 120.0],\n",
        "             [-84.85,84.85],\n",
        "             [-120.0, 0],\n",
        "             [-84.85, -84.85],\n",
        "             [0.0, -120],\n",
        "             [84.85,-84.85]])"
      ],
      "metadata": {
        "id": "sHVkZ4lKoiEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "middle_reach_dict = {\n",
        "    0.0: 0.0,\n",
        "    -84.85: 84.85,\n",
        "    84.85: -84.85,\n",
        "    -120.0: 120.0,\n",
        "    120.0: -120.0,\n",
        "}"
      ],
      "metadata": {
        "id": "R4wxmyD0ztFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_reach_trials = 505 # reaches not including the first\n",
        "# n_reach_trials = 253 # reaches besides middle\n",
        "spike_times = np.ndarray(shape=(n_reach_trials, num_electrodes, ms))\n",
        "targets = np.ndarray(shape=(n_reach_trials, 1), dtype=int)\n",
        "count = {}\n",
        "per_reach = {}\n",
        "total = 0\n",
        "prev = (round(R[0]['target'][0].item(), 2), round(R[0]['target'][1].item(), 2))\n",
        "for n in range(ntrials):\n",
        "    spike_time = R[n]['spikeRaster'].todense()[:, ms_start: ms_start + ms]\n",
        "    target_x = round(R[n]['target'][0].item(), 2)\n",
        "    target_y = round(R[n]['target'][1].item(), 2)\n",
        "    # first check if middle\n",
        "    if target_x == round(0.0, 2) and target_y == round(0.0, 2):\n",
        "        target_x = round(middle_reach_dict[prev[0]], 2)\n",
        "        target_y = round(middle_reach_dict[prev[1]], 2)\n",
        "    for d, dir in enumerate(xy_sorted):\n",
        "        if target_x == round(dir[0], 2) and target_y == round(dir[1], 2):\n",
        "            prev = (target_x, target_y)\n",
        "            if d not in count:\n",
        "                count[d] = 0\n",
        "                per_reach[d] = []\n",
        "            per_reach[d].append(spike_time)\n",
        "            count[d] += 1\n",
        "            # col[d] = 1\n",
        "            targets[total, :] = d\n",
        "            spike_times[total, :, :] = spike_time\n",
        "            total += 1\n",
        "            break\n",
        "spike_times.shape, targets.shape, total"
      ],
      "metadata": {
        "id": "l0YARmeXD6u9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6256a9ff-649d-444d-852d-7c51489844b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((505, 96, 400), (505, 1), 505)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count # check distribution of data"
      ],
      "metadata": {
        "id": "qkYDLbj35J8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8d702b-239a-4f4e-8afc-3d349f46725b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{6: 63, 2: 63, 5: 63, 1: 62, 0: 64, 4: 64, 3: 63, 7: 63}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = .8\n",
        "total = 0\n",
        "for c in count:\n",
        "    total += int(train_size * count[c])\n",
        "total"
      ],
      "metadata": {
        "id": "rA0-9l-o4QlN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb51934-38df-4394-f280-71189ded5bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "401"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evenly Sample From Each Class"
      ],
      "metadata": {
        "id": "8PDeaYGQW-Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.model_selection as sk\n",
        "\n",
        "# num_train = 173\n",
        "num_train = total\n",
        "# num_train = int(n_reach_trials * .7)\n",
        "X_train = np.ndarray(shape=(num_train, num_electrodes, ms))\n",
        "X_test = np.ndarray(shape=(n_reach_trials - num_train, num_electrodes, ms))\n",
        "\n",
        "Y_train = np.ndarray(shape=(num_train, 1), dtype=int)\n",
        "Y_test = np.ndarray(shape=(n_reach_trials - num_train, 1), dtype=int)\n",
        "train_start = 0\n",
        "test_start = 0\n",
        "for d in per_reach:\n",
        "    reach = np.array(per_reach[d])\n",
        "    target = np.full((reach.shape[0], 1), d)\n",
        "    reach_x_train, reach_x_test, reach_y_train, reach_y_test = sk.train_test_split(reach, target, train_size=train_size, random_state = 42)\n",
        "    print(reach_x_train.shape[0], reach_x_test.shape[0])\n",
        "    X_train[train_start: train_start + reach_x_train.shape[0], :, :] = reach_x_train\n",
        "    X_test[test_start: test_start + reach_x_test.shape[0], :, :] = reach_x_test\n",
        "    Y_train[train_start: train_start + reach_y_train.shape[0], :] = reach_y_train\n",
        "    Y_test[test_start: test_start + reach_y_test.shape[0], :] = reach_y_test\n",
        "\n",
        "    train_start += reach_x_train.shape[0]\n",
        "    test_start += reach_x_test.shape[0]\n"
      ],
      "metadata": {
        "id": "xTgNLEVXE8pH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f88d9a8-7e71-4de2-8cd5-c8894fad5075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 13\n",
            "50 13\n",
            "50 13\n",
            "49 13\n",
            "51 13\n",
            "51 13\n",
            "50 13\n",
            "50 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Data to `tf.data.Dataset`"
      ],
      "metadata": {
        "id": "GdvJ74A5XnmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.cast(X_train, tf.float32)\n",
        "X_test = tf.cast(X_test, tf.float32)\n",
        "X_train2 = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "X_test2 = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "Y_train2 = tf.data.Dataset.from_tensor_slices(Y_train)\n",
        "Y_test2 = tf.data.Dataset.from_tensor_slices(Y_test)"
      ],
      "metadata": {
        "id": "lSGLjkqWJnQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Map Functions to Data"
      ],
      "metadata": {
        "id": "G7lZJcdVXZnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_matrix(label, depth=8):\n",
        "    \"\"\"\n",
        "    Computes the one hot encoding for a single label\n",
        "    \n",
        "    Arguments:\n",
        "        label --  (int) Categorical labels\n",
        "        depth --  (int) Number of different classes that label can take\n",
        "    \n",
        "    Returns:\n",
        "         one_hot -- tf.Tensor A single-column matrix with the one hot encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    one_hot = tf.reshape(tf.one_hot(label, depth, axis=0), (depth,))\n",
        "    return one_hot\n",
        "\n",
        "def normalize(x):\n",
        "    \"\"\"\n",
        "    Transform an image into a tensor of shape (96 * 300, )\n",
        "    and normalize its components.\n",
        "    \n",
        "    Arguments\n",
        "    image - Tensor.\n",
        "    \n",
        "    Returns: \n",
        "    result -- Transformed tensor \n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, [-1,])\n",
        "    return x"
      ],
      "metadata": {
        "id": "7-kA1uo2s8Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = X_train2.map(normalize) # flatten signals\n",
        "x_test = X_test2.map(normalize)"
      ],
      "metadata": {
        "id": "VViXzJUsCS01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = Y_train2.map(one_hot_matrix) # convert classes to one hot\n",
        "y_test = Y_test2.map(one_hot_matrix)"
      ],
      "metadata": {
        "id": "FDU3mgVd1DtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_train), len(x_train)"
      ],
      "metadata": {
        "id": "IAfwzaT9xRud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff02822-7dde-4ef5-81db-190ed712f9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401, 401)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(next(iter(y_test)))"
      ],
      "metadata": {
        "id": "3x32TtA988eD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309052bd-9624-49f7-c8f3-2426e2d3b1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 0. 0. 0. 1. 0.], shape=(8,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model"
      ],
      "metadata": {
        "id": "wxsImUf6W6dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "xLQdkhMGXTvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    Initializes parameters to build a neural network with TensorFlow. The shapes are:\n",
        "                        W1 : [40, 28800]\n",
        "                        b1 : [40, 1]\n",
        "                        W2 : [12, 40]\n",
        "                        b2 : [12, 1]\n",
        "                        W3 : [8, 12]\n",
        "                        b3 : [8, 1]\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
        "    \"\"\"\n",
        "                                \n",
        "    initializer = tf.keras.initializers.GlorotNormal(seed=1)   \n",
        "\n",
        "    W1 = tf.Variable(initializer(shape=(40, START_SIZE)))\n",
        "    b1 = tf.Variable(initializer(shape=(40, 1)))\n",
        "    W2 = tf.Variable(initializer(shape=(12, 40)))\n",
        "    b2 = tf.Variable(initializer(shape=(12, 1)))\n",
        "    W3 = tf.Variable(initializer(shape=(8, 12)))\n",
        "    b3 = tf.Variable(initializer(shape=(8, 1)))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2,\n",
        "                  \"W3\": W3,\n",
        "                  \"b3\": b3}\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3']\n",
        "    \n",
        "    Z1 = tf.math.add(tf.linalg.matmul(W1,  X), b1)\n",
        "    A1 = tf.keras.activations.relu(Z1)\n",
        "    Z2 = tf.math.add(tf.linalg.matmul(W2, A1), b2)\n",
        "    A2 = tf.keras.activations.relu(Z2)\n",
        "    Z3 = tf.math.add(tf.linalg.matmul(W3, A2), b3)\n",
        "\n",
        "    \n",
        "    return Z3\n",
        "\n",
        "def compute_cost(logits, labels):\n",
        "    \"\"\"\n",
        "    Computes the cost\n",
        "    \n",
        "    Arguments:\n",
        "    logits -- output of forward propagation (output of the last LINEAR unit), of shape (8, num_examples)\n",
        "    labels -- \"true\" labels vector, same shape as Z3\n",
        "    \n",
        "    Returns:\n",
        "    cost - Tensor of the cost function\n",
        "    \"\"\"\n",
        "    logits = tf.transpose(logits)\n",
        "    labels = tf.transpose(labels)\n",
        "    cost = tf.math.reduce_sum(tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=True))\n",
        "    # cost = tf.math.reduce_sum(tf.keras.losses.binary_crossentropy(labels, logits, from_logits=True))\n",
        "    return cost"
      ],
      "metadata": {
        "id": "xin6ZGf39cOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "6YAAaWl9XWcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
        "          num_epochs = 1500, minibatch_size = 4, print_cost = True):\n",
        "    \n",
        "    costs = []                                        # To keep track of the cost\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "  \n",
        "    parameters = initialize_parameters()\n",
        "\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3']\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    #f1 score + confusion metrics\n",
        "    \n",
        "    # The CategoricalAccuracy will track the accuracy for this multiclass problem\n",
        "    test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    print(len(X_train), len(Y_train), X_train, Y_train)\n",
        "    dataset = tf.data.Dataset.zip((X_train, Y_train))\n",
        "    test_dataset = tf.data.Dataset.zip((X_test, Y_test))\n",
        "    \n",
        "    m = dataset.cardinality().numpy()\n",
        "    \n",
        "    minibatches = dataset.batch(minibatch_size).prefetch(8)\n",
        "    test_minibatches = test_dataset.batch(minibatch_size).prefetch(8)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        epoch_cost = 0.\n",
        "        \n",
        "        train_accuracy.reset_states()\n",
        "        \n",
        "        for (minibatch_X, minibatch_Y) in minibatches:\n",
        "            \n",
        "            with tf.GradientTape() as tape:\n",
        "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
        "                minibatch_cost = compute_cost(Z3, tf.transpose(minibatch_Y))\n",
        "\n",
        "            train_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
        "            \n",
        "            trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
        "            grads = tape.gradient(minibatch_cost, trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, trainable_variables))\n",
        "            epoch_cost += minibatch_cost\n",
        "        \n",
        "        epoch_cost /= m\n",
        "\n",
        "        if print_cost == True and epoch % 10 == 0:\n",
        "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "            print(\"Train accuracy:\", train_accuracy.result())\n",
        "            \n",
        "            for (minibatch_X, minibatch_Y) in test_minibatches:\n",
        "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
        "                test_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
        "            print(\"Test_accuracy:\", test_accuracy.result())\n",
        "\n",
        "            costs.append(epoch_cost)\n",
        "            train_acc.append(train_accuracy.result())\n",
        "            test_acc.append(test_accuracy.result())\n",
        "            test_accuracy.reset_states()\n",
        "\n",
        "\n",
        "    return parameters, costs, train_acc, test_acc"
      ],
      "metadata": {
        "id": "wPuD3JiF9T3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "zbXNJgtcWwEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, costs, train_acc, test_acc = model(x_train, y_train, x_test, y_test, num_epochs=110)"
      ],
      "metadata": {
        "id": "zQWezvRc979v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "c2327c2b-4786-4486-a8e2-c54d582c446a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401 401 <MapDataset element_spec=TensorSpec(shape=(28800,), dtype=tf.float32, name=None)> <MapDataset element_spec=TensorSpec(shape=(8,), dtype=tf.float32, name=None)>\n",
            "Cost after epoch 0: 2.228898\n",
            "Train accuracy: tf.Tensor(0.13965087, shape=(), dtype=float32)\n",
            "Test_accuracy: tf.Tensor(0.13461539, shape=(), dtype=float32)\n",
            "Cost after epoch 10: 0.304657\n",
            "Train accuracy: tf.Tensor(0.9725686, shape=(), dtype=float32)\n",
            "Test_accuracy: tf.Tensor(0.21153846, shape=(), dtype=float32)\n",
            "Cost after epoch 20: 0.032504\n",
            "Train accuracy: tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "Test_accuracy: tf.Tensor(0.21153846, shape=(), dtype=float32)\n",
            "Cost after epoch 30: 0.008805\n",
            "Train accuracy: tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "Test_accuracy: tf.Tensor(0.20192307, shape=(), dtype=float32)\n",
            "Cost after epoch 40: 0.003429\n",
            "Train accuracy: tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "Test_accuracy: tf.Tensor(0.21153846, shape=(), dtype=float32)\n",
            "Cost after epoch 50: 0.001497\n",
            "Train accuracy: tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "Test_accuracy: tf.Tensor(0.21153846, shape=(), dtype=float32)\n",
            "Cost after epoch 60: 0.000710\n",
            "Train accuracy: tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "Test_accuracy: tf.Tensor(0.21153846, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e6048d2ee8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-f3b97fadcfce>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mZ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mminibatch_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-3a0b2fc1775c>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mb3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3711\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[1;32m   3712\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3713\u001b[0;31m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3714\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[1;32m   3715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6011\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6012\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6013\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   6014\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6015\u001b[0m         transpose_b)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "np.random.seed(0)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "id": "GA2gb0Hkv_aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: Emojify_V2\n",
        "\n",
        "def Neuro(input_shape):\n",
        "    \"\"\"\n",
        "    Function creating the Emojify-v2 model's graph.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the input, usually (max_len,)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    model -- a model instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Define sentence_indices as the input of the graph.\n",
        "    # It should be of shape input_shape and dtype 'int32' (as it contains indices, which are integers).\n",
        "    print(input_shape)\n",
        "    input = Input(input_shape, dtype='float32')\n",
        "    print(input.shape)\n",
        "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a batch of sequences.\n",
        "    X = LSTM(128, return_sequences=True)(input)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(.7)(X) \n",
        "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a single hidden state, not a batch of sequences.\n",
        "    X = LSTM(64, return_sequences=False)(X)\n",
        "    # # # Add dropout with a probability of 0.5\n",
        "    # X = Dropout(.7)(X)\n",
        "    # X = LSTM(32, return_sequences=False)(X)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(.7)(X)  \n",
        "    # Propagate X through a Dense layer with 5 units\n",
        "    X = Dense(8)(X) \n",
        "    # Add a softmax activation\n",
        "    X = Activation('softmax')(X)\n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = Model(input, X)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "Sv2Fdt1g4tFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Neuro((ms, num_electrodes))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "u5OYFh8MwAJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5781932-bdad-46df-b10f-d5626a39b6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(400, 96)\n",
            "(None, 400, 96)\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 400, 96)]         0         \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 400, 128)          115200    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 400, 128)          0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 520       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 165,128\n",
            "Trainable params: 165,128\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2krqeE6ewawx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "id": "SN-wiGozRMKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb43c743-ddb4-430a-843c-3ca2f302be34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_LSTM = []\n",
        "for t in range(Y_train.shape[0]):\n",
        "    col = np.zeros((8,))\n",
        "    col[Y_train[t][0]] = 1\n",
        "    Y_train_LSTM.append(col)\n",
        "Y_train_LSTM = np.array(Y_train_LSTM)\n",
        "\n",
        "Y_test_LSTM = []\n",
        "for t in range(Y_test.shape[0]):\n",
        "    col = np.zeros((8,))\n",
        "    col[Y_test[t][0]] = 1\n",
        "    Y_test_LSTM.append(col)\n",
        "Y_test_LSTM = np.array(Y_test_LSTM)\n",
        "\n",
        "Y_train_LSTM.shape, Y_test_LSTM.shape"
      ],
      "metadata": {
        "id": "wysRhnArN7bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3109039b-0b6c-47d5-de48-ae6b51e42525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((401, 8), (104, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_LSTM = tf.reshape(X_train, (X_train.shape[0], X_train.shape[2], X_train.shape[1]))\n",
        "X_test_LSTM = tf.reshape(X_test, (X_test.shape[0], X_test.shape[2], X_test.shape[1]))"
      ],
      "metadata": {
        "id": "820MQmBPR-TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_LSTM.shape, Y_train_LSTM.shape, X_test_LSTM.shape, Y_test_LSTM.shape"
      ],
      "metadata": {
        "id": "y1QizCu30cPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf06ce5-3000-4ecf-941f-5c1038ae34ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([401, 400, 96]), (401, 8), TensorShape([104, 400, 96]), (104, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_LSTM[0][0]"
      ],
      "metadata": {
        "id": "XthfPiO3ZPAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484f0758-dc57-4bb2-87ff-c3b53e5c9997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(96,), dtype=float32, numpy=\n",
              "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_LSTM, Y_train_LSTM, epochs = 60, batch_size = 32, shuffle=True)"
      ],
      "metadata": {
        "id": "S5cRLxPN0GkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94746f6a-4cd7-4ad1-c632-ec2083f984ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "13/13 [==============================] - 10s 519ms/step - loss: 2.0809 - accuracy: 0.1172\n",
            "Epoch 2/60\n",
            "13/13 [==============================] - 7s 507ms/step - loss: 2.0746 - accuracy: 0.1322\n",
            "Epoch 3/60\n",
            "13/13 [==============================] - 7s 515ms/step - loss: 2.0636 - accuracy: 0.1496\n",
            "Epoch 4/60\n",
            "13/13 [==============================] - 7s 512ms/step - loss: 2.0574 - accuracy: 0.1796\n",
            "Epoch 5/60\n",
            "13/13 [==============================] - 7s 514ms/step - loss: 2.0425 - accuracy: 0.2195\n",
            "Epoch 6/60\n",
            "13/13 [==============================] - 7s 512ms/step - loss: 2.0089 - accuracy: 0.2344\n",
            "Epoch 7/60\n",
            "13/13 [==============================] - 7s 513ms/step - loss: 1.9503 - accuracy: 0.2594\n",
            "Epoch 8/60\n",
            "13/13 [==============================] - 7s 515ms/step - loss: 1.8771 - accuracy: 0.2668\n",
            "Epoch 9/60\n",
            "13/13 [==============================] - 7s 514ms/step - loss: 1.8228 - accuracy: 0.2743\n",
            "Epoch 10/60\n",
            "13/13 [==============================] - 7s 510ms/step - loss: 1.7826 - accuracy: 0.2993\n",
            "Epoch 11/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 1.7457 - accuracy: 0.3217\n",
            "Epoch 12/60\n",
            "13/13 [==============================] - 7s 512ms/step - loss: 1.6743 - accuracy: 0.3691\n",
            "Epoch 13/60\n",
            "13/13 [==============================] - 7s 512ms/step - loss: 1.6558 - accuracy: 0.3666\n",
            "Epoch 14/60\n",
            "13/13 [==============================] - 8s 571ms/step - loss: 1.5697 - accuracy: 0.4339\n",
            "Epoch 15/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 1.5195 - accuracy: 0.4040\n",
            "Epoch 16/60\n",
            "13/13 [==============================] - 7s 510ms/step - loss: 1.4850 - accuracy: 0.4464\n",
            "Epoch 17/60\n",
            "13/13 [==============================] - 7s 513ms/step - loss: 1.4471 - accuracy: 0.4763\n",
            "Epoch 18/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 1.4346 - accuracy: 0.4539\n",
            "Epoch 19/60\n",
            "13/13 [==============================] - 7s 510ms/step - loss: 1.4081 - accuracy: 0.4938\n",
            "Epoch 20/60\n",
            "13/13 [==============================] - 7s 514ms/step - loss: 1.3849 - accuracy: 0.4938\n",
            "Epoch 21/60\n",
            "13/13 [==============================] - 7s 509ms/step - loss: 1.3034 - accuracy: 0.5611\n",
            "Epoch 22/60\n",
            "13/13 [==============================] - 7s 516ms/step - loss: 1.2570 - accuracy: 0.5387\n",
            "Epoch 23/60\n",
            "13/13 [==============================] - 7s 512ms/step - loss: 1.2007 - accuracy: 0.5611\n",
            "Epoch 24/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 1.1453 - accuracy: 0.6060\n",
            "Epoch 25/60\n",
            "13/13 [==============================] - 7s 514ms/step - loss: 1.1255 - accuracy: 0.6160\n",
            "Epoch 26/60\n",
            "13/13 [==============================] - 7s 516ms/step - loss: 1.0835 - accuracy: 0.6209\n",
            "Epoch 27/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 1.0324 - accuracy: 0.6309\n",
            "Epoch 28/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 0.9493 - accuracy: 0.6783\n",
            "Epoch 29/60\n",
            "13/13 [==============================] - 7s 517ms/step - loss: 0.9091 - accuracy: 0.7032\n",
            "Epoch 30/60\n",
            "13/13 [==============================] - 7s 512ms/step - loss: 1.0201 - accuracy: 0.6608\n",
            "Epoch 31/60\n",
            "13/13 [==============================] - 7s 515ms/step - loss: 0.9303 - accuracy: 0.6983\n",
            "Epoch 32/60\n",
            "13/13 [==============================] - 7s 513ms/step - loss: 0.9169 - accuracy: 0.7082\n",
            "Epoch 33/60\n",
            "13/13 [==============================] - 7s 514ms/step - loss: 0.8696 - accuracy: 0.7307\n",
            "Epoch 34/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 0.8036 - accuracy: 0.7406\n",
            "Epoch 35/60\n",
            "13/13 [==============================] - 7s 510ms/step - loss: 0.7129 - accuracy: 0.7980\n",
            "Epoch 36/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 0.6736 - accuracy: 0.7980\n",
            "Epoch 37/60\n",
            "13/13 [==============================] - 7s 505ms/step - loss: 0.7891 - accuracy: 0.7855\n",
            "Epoch 38/60\n",
            "13/13 [==============================] - 7s 515ms/step - loss: 0.6965 - accuracy: 0.7855\n",
            "Epoch 39/60\n",
            "13/13 [==============================] - 7s 515ms/step - loss: 0.7586 - accuracy: 0.7431\n",
            "Epoch 40/60\n",
            "13/13 [==============================] - 8s 590ms/step - loss: 0.6391 - accuracy: 0.8030\n",
            "Epoch 41/60\n",
            "13/13 [==============================] - 7s 515ms/step - loss: 1.5408 - accuracy: 0.6658\n",
            "Epoch 42/60\n",
            "13/13 [==============================] - 7s 514ms/step - loss: 0.9727 - accuracy: 0.7007\n",
            "Epoch 43/60\n",
            "13/13 [==============================] - 7s 510ms/step - loss: 0.8594 - accuracy: 0.7232\n",
            "Epoch 44/60\n",
            "13/13 [==============================] - 7s 512ms/step - loss: 0.6903 - accuracy: 0.7830\n",
            "Epoch 45/60\n",
            "13/13 [==============================] - 7s 515ms/step - loss: 0.6056 - accuracy: 0.7930\n",
            "Epoch 46/60\n",
            "13/13 [==============================] - 7s 504ms/step - loss: 0.5892 - accuracy: 0.8354\n",
            "Epoch 47/60\n",
            "13/13 [==============================] - 7s 506ms/step - loss: 0.6022 - accuracy: 0.8279\n",
            "Epoch 48/60\n",
            "13/13 [==============================] - 7s 510ms/step - loss: 0.5527 - accuracy: 0.8429\n",
            "Epoch 49/60\n",
            "13/13 [==============================] - 7s 510ms/step - loss: 0.5073 - accuracy: 0.8653\n",
            "Epoch 50/60\n",
            "13/13 [==============================] - 7s 516ms/step - loss: 0.4670 - accuracy: 0.8529\n",
            "Epoch 51/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 0.4523 - accuracy: 0.8703\n",
            "Epoch 52/60\n",
            "13/13 [==============================] - 7s 506ms/step - loss: 0.4870 - accuracy: 0.8579\n",
            "Epoch 53/60\n",
            "13/13 [==============================] - 7s 511ms/step - loss: 0.4379 - accuracy: 0.8903\n",
            "Epoch 54/60\n",
            "13/13 [==============================] - 7s 512ms/step - loss: 0.3770 - accuracy: 0.8878\n",
            "Epoch 55/60\n",
            "13/13 [==============================] - 7s 512ms/step - loss: 0.3578 - accuracy: 0.8978\n",
            "Epoch 56/60\n",
            "13/13 [==============================] - 7s 520ms/step - loss: 0.5389 - accuracy: 0.8379\n",
            "Epoch 57/60\n",
            "13/13 [==============================] - 7s 518ms/step - loss: 0.5371 - accuracy: 0.8479\n",
            "Epoch 58/60\n",
            "13/13 [==============================] - 9s 658ms/step - loss: 0.4388 - accuracy: 0.8828\n",
            "Epoch 59/60\n",
            "13/13 [==============================] - 7s 521ms/step - loss: 0.3525 - accuracy: 0.9027\n",
            "Epoch 60/60\n",
            "13/13 [==============================] - 7s 519ms/step - loss: 0.2865 - accuracy: 0.9227\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6c66faf70>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(X_test_LSTM, Y_test_LSTM)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "metadata": {
        "id": "sFw5X0ey0e_l",
        "outputId": "e93a4273-72f5-4a74-aea6-bf83ec3535e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 2s 370ms/step - loss: 4.4854 - accuracy: 0.1250\n",
            "\n",
            "Test accuracy =  0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lw5NECinWwiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}